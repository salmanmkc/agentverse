{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine-tuning GPT-OSS 20B Model for Persona Emulation\n",
        "\n",
        "This notebook fine-tunes GPT-OSS 20B model using Gmail dataset to emulate your persona by learning your email writing patterns.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports and Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "from unsloth.chat_templates import standardize_sharegpt, train_on_responses_only\n",
        "from transformers import TextStreamer\n",
        "from trl import SFTConfig, SFTTrainer\n",
        "from datasets import load_dataset, load_from_disk\n",
        "from pathlib import Path\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "\n",
        "Set your parameters here:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset configuration\n",
        "hf_repo_id = \"ryanlin10/gmail_dataset\"  # HuggingFace repository ID\n",
        "hf_token = os.environ.get(\"HF_TOKEN\", None)  # Or set directly: \"your_token_here\"\n",
        "local_path = None  # Set to local path if loading from disk, e.g., \"./gmail_dataset\"\n",
        "\n",
        "# Model configuration\n",
        "model_name = \"unsloth/gpt-oss-20b\"\n",
        "max_seq_length = 1024\n",
        "\n",
        "# Output configuration\n",
        "output_dir = \"outputs/gpt_oss_finetuned\"\n",
        "save_model = True\n",
        "\n",
        "# LoRA configuration\n",
        "lora_r = 8\n",
        "lora_alpha = 16\n",
        "\n",
        "# Training configuration\n",
        "num_train_epochs = 1\n",
        "max_steps = None  # Set to override epochs, e.g., 100\n",
        "per_device_batch_size = 1\n",
        "gradient_accumulation_steps = 4\n",
        "learning_rate = 2e-4\n",
        "warmup_steps = 5\n",
        "\n",
        "# Testing configuration\n",
        "test_inference = True  # Set to True to test inference after training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset Transformation Functions\n",
        "\n",
        "The Gmail dataset has a single system message with all context and email content. We split it into user/assistant format where:\n",
        "- User message contains context (recipient, subject, original email if reply)\n",
        "- Assistant message contains your actual email content (what the model learns)\n",
        "\n",
        "No AI assistant system prompts or \"write an email\" instructions are included.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def transform_gmail_to_gpt_oss_format(example):\n",
        "    \"\"\"\n",
        "    Transform Gmail dataset format to GPT-OSS format for persona emulation.\n",
        "    The Gmail dataset has a single system message with all context and email content.\n",
        "    We split it into user/assistant format where:\n",
        "    - User message contains context (recipient, subject, original email if reply)\n",
        "    - Assistant message contains your actual email content (what the model learns)\n",
        "    \n",
        "    No AI assistant system prompts or \"write an email\" instructions are included.\n",
        "    \"\"\"\n",
        "    # Safely access messages and metadata (handle missing keys)\n",
        "    original_messages = example.get(\"messages\", [])\n",
        "    metadata = example.get(\"metadata\", {})\n",
        "    \n",
        "    # If messages is missing or empty, skip this example\n",
        "    if not original_messages:\n",
        "        return {\"messages\": []}\n",
        "    \n",
        "    # The Gmail dataset currently has only a system message with all content\n",
        "    if len(original_messages) == 1 and original_messages[0].get(\"role\") == \"system\":\n",
        "        system_content = original_messages[0].get(\"content\", \"\")\n",
        "        \n",
        "        # Extract email content and context\n",
        "        # The Gmail format is: \"You are writing an email. Context: ... --- Your reply to the original email ---\\n{email_content}\"\n",
        "        # This marker is present for ALL emails (reply or not), so we can reliably split on it\n",
        "        email_content = \"\"\n",
        "        context = \"\"\n",
        "        \n",
        "        if \"--- Your reply to the original email ---\" in system_content:\n",
        "            parts = system_content.split(\"--- Your reply to the original email ---\", 1)\n",
        "            context = parts[0].strip()\n",
        "            email_content = parts[1].strip() if len(parts) > 1 else \"\"\n",
        "        else:\n",
        "            # Fallback: if marker is missing (shouldn't happen), try to extract from end\n",
        "            # Look for the actual email content (usually starts after Context info)\n",
        "            lines = system_content.split('\\n')\n",
        "            # Find where context likely ends\n",
        "            context_lines = []\n",
        "            content_start = len(lines)\n",
        "            for i, line in enumerate(lines):\n",
        "                if line.strip().startswith('---') or (line.strip() and not ('Context:' in line or 'Recipient:' in line or 'Subject:' in line or 'Date:' in line or 'You are writing' in line)):\n",
        "                    # This might be the start of actual email content\n",
        "                    content_start = i\n",
        "                    break\n",
        "                context_lines.append(line)\n",
        "            \n",
        "            context = '\\n'.join(context_lines).strip()\n",
        "            email_content = '\\n'.join(lines[content_start:]).strip() if content_start < len(lines) else system_content\n",
        "        \n",
        "        # Get metadata\n",
        "        subject = metadata.get(\"subject\", \"No Subject\")\n",
        "        recipient = metadata.get(\"recipient\", \"Unknown Recipient\")\n",
        "        is_reply = metadata.get(\"is_reply\", False)\n",
        "        \n",
        "        # Build messages in GPT-OSS format - natural context without AI assistant prompts\n",
        "        new_messages = []\n",
        "        \n",
        "        # Add context naturally - just the facts without instructional prompts\n",
        "        if is_reply and \"--- Original Email ---\" in context:\n",
        "            # Extract the original email context for replies\n",
        "            orig_email_start = context.find(\"--- Original Email ---\")\n",
        "            if orig_email_start != -1:\n",
        "                orig_email_section = context[orig_email_start:]\n",
        "                # Clean up headers but keep the email structure\n",
        "                # The original email section already contains \"From:\", \"Subject:\", \"Date:\", and \"Content:\"\n",
        "                # Remove only the instructional markers\n",
        "                orig_email_section = orig_email_section.replace(\"--- Original Email ---\", \"\").replace(\"--- End Original Email ---\", \"\").strip()\n",
        "                # Clean up any \"This is a reply to...\" instructional text if present\n",
        "                if \"This is a reply to\" in orig_email_section:\n",
        "                    lines = orig_email_section.split('\\n')\n",
        "                    cleaned_lines = []\n",
        "                    skip_instruction = True\n",
        "                    for line in lines:\n",
        "                        if \"--- Original Email ---\" not in line and \"This is a reply to\" not in line:\n",
        "                            if skip_instruction and line.strip():\n",
        "                                skip_instruction = False\n",
        "                            if not skip_instruction:\n",
        "                                cleaned_lines.append(line)\n",
        "                    orig_email_section = '\\n'.join(cleaned_lines).strip()\n",
        "                \n",
        "                new_messages.append({\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": f\"Replying to email:\\n{orig_email_section}\"\n",
        "                })\n",
        "        else:\n",
        "            # For new emails, just provide recipient and subject as context\n",
        "            new_messages.append({\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"To: {recipient}\\nSubject: {subject}\"\n",
        "            })\n",
        "        \n",
        "        # Add assistant response (your actual email content - this is what the model learns to emulate)\n",
        "        # Clean up email_content if needed\n",
        "        if not email_content or not email_content.strip():\n",
        "            # If we couldn't extract email content, mark for filtering\n",
        "            return {\"messages\": []}  # Empty messages will be filtered out\n",
        "        \n",
        "        new_messages.append({\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": email_content\n",
        "        })\n",
        "        \n",
        "        return {\"messages\": new_messages}\n",
        "    \n",
        "    # If already in correct format, return as is\n",
        "    return {\"messages\": original_messages}\n",
        "\n",
        "\n",
        "def formatting_prompts_func(examples, tokenizer):\n",
        "    \"\"\"Format conversations into text using the tokenizer's chat template.\"\"\"\n",
        "    convos = examples[\"messages\"]\n",
        "    texts = [tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False) for convo in convos]\n",
        "    return {\"text\": texts}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_gmail_dataset(dataset_path=None, hf_repo_id=None, hf_token=None, local_path=None):\n",
        "    \"\"\"\n",
        "    Load Gmail dataset from either HuggingFace Hub or local path.\n",
        "    \n",
        "    Args:\n",
        "        dataset_path: Local path to dataset folder (deprecated, use local_path)\n",
        "        hf_repo_id: HuggingFace repository ID (e.g., \"ryanlin10/gmail_dataset\")\n",
        "        hf_token: HuggingFace token (if needed for private datasets)\n",
        "        local_path: Local path to dataset folder\n",
        "    \n",
        "    Returns:\n",
        "        Dataset object\n",
        "    \"\"\"\n",
        "    # Use local_path if provided, otherwise fall back to dataset_path for backward compatibility\n",
        "    local_path = local_path or dataset_path\n",
        "    \n",
        "    if hf_repo_id:\n",
        "        print(f\"Loading dataset from HuggingFace: {hf_repo_id}\")\n",
        "        if hf_token:\n",
        "            dataset = load_dataset(hf_repo_id, token=hf_token)\n",
        "            # Handle case where dataset might have splits\n",
        "            if isinstance(dataset, dict):\n",
        "                dataset = dataset.get(\"train\", list(dataset.values())[0])\n",
        "        else:\n",
        "            dataset = load_dataset(hf_repo_id)\n",
        "            if isinstance(dataset, dict):\n",
        "                dataset = dataset.get(\"train\", list(dataset.values())[0])\n",
        "        print(f\"‚úì Loaded Gmail dataset from HF with {len(dataset)} examples\")\n",
        "    elif local_path and Path(local_path).exists():\n",
        "        print(f\"Loading dataset from local path: {local_path}\")\n",
        "        dataset = load_from_disk(local_path)\n",
        "        print(f\"‚úì Loaded Gmail dataset locally with {len(dataset)} examples\")\n",
        "    else:\n",
        "        raise FileNotFoundError(\n",
        "            f\"Dataset not found. Please provide either:\\n\"\n",
        "            f\"  1. HuggingFace repo_id (e.g., 'ryanlin10/gmail_dataset') with optional token\\n\"\n",
        "            f\"  2. Local path to dataset folder\"\n",
        "        )\n",
        "    \n",
        "    return dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Main Execution\n",
        "\n",
        "### Step 1: Load Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"GPT-OSS 20B Fine-tuning for Persona Emulation\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\nüìä Step 1: Loading Gmail dataset...\")\n",
        "dataset = load_gmail_dataset(\n",
        "    hf_repo_id=hf_repo_id if not local_path else None,\n",
        "    hf_token=hf_token,\n",
        "    local_path=local_path\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Transform Dataset to GPT-OSS Format\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nüîÑ Step 2: Transforming dataset to GPT-OSS format...\")\n",
        "print(f\"Dataset columns: {dataset.column_names}\")\n",
        "if len(dataset) > 0:\n",
        "    print(f\"First example keys: {list(dataset[0].keys())}\")\n",
        "\n",
        "dataset = dataset.map(\n",
        "    transform_gmail_to_gpt_oss_format,\n",
        "    remove_columns=[col for col in dataset.column_names if col != \"messages\"]\n",
        ")\n",
        "\n",
        "# Filter out examples without valid messages\n",
        "dataset = dataset.filter(lambda x: \"messages\" in x and x[\"messages\"] is not None and len(x[\"messages\"]) > 0)\n",
        "print(f\"After transformation: {len(dataset)} examples\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Standardize Dataset Format\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nüìù Step 3: Standardizing dataset format...\")\n",
        "dataset = standardize_sharegpt(dataset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4: Load Model and Tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"\\nü§ñ Step 4: Loading model {model_name}...\")\n",
        "dtype = None  # Auto-detect\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=model_name,\n",
        "    dtype=dtype,\n",
        "    max_seq_length=max_seq_length,\n",
        "    load_in_4bit=True,  # 4 bit quantization to reduce memory\n",
        "    full_finetuning=False,\n",
        "    token=hf_token if model_name.startswith((\"hf_\", \"openai/\")) else None,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 5: Add LoRA Adapters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"\\nüîß Step 5: Adding LoRA adapters (r={lora_r}, alpha={lora_alpha})...\")\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=lora_r,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                   \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha=lora_alpha,\n",
        "    lora_dropout=0,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state=3407,\n",
        "    use_rslora=False,\n",
        "    loftq_config=None,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 6: Format Prompts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nüìã Step 6: Formatting prompts...\")\n",
        "formatting_func = lambda examples: formatting_prompts_func(examples, tokenizer)\n",
        "dataset = dataset.map(formatting_func, batched=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 7: Setup Trainer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nüöÄ Step 7: Setting up trainer...\")\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    args=SFTConfig(\n",
        "        per_device_train_batch_size=per_device_batch_size,\n",
        "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "        warmup_steps=warmup_steps,\n",
        "        num_train_epochs=num_train_epochs if max_steps is None else None,\n",
        "        max_steps=max_steps,\n",
        "        learning_rate=learning_rate,\n",
        "        logging_steps=1,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.001,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=3407,\n",
        "        output_dir=output_dir,\n",
        "        report_to=\"none\",\n",
        "    ),\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 8: Configure Response-Only Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nüéØ Step 8: Configuring response-only training...\")\n",
        "gpt_oss_kwargs = dict(\n",
        "    instruction_part=\"<|start|>user<|message|>\",\n",
        "    response_part=\"<|start|>assistant<|channel|>final<|message|>\"\n",
        ")\n",
        "trainer = train_on_responses_only(trainer, **gpt_oss_kwargs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 9: Display Training Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nüìä Training Configuration:\")\n",
        "gpu_stats = torch.cuda.get_device_properties(0) if torch.cuda.is_available() else None\n",
        "if gpu_stats:\n",
        "    start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "    max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "    print(f\"  GPU = {gpu_stats.name}\")\n",
        "    print(f\"  Max memory = {max_memory} GB\")\n",
        "    print(f\"  Reserved memory = {start_gpu_memory} GB\")\n",
        "print(f\"  Training examples = {len(dataset)}\")\n",
        "print(f\"  Batch size = {per_device_batch_size} x {gradient_accumulation_steps}\")\n",
        "if max_steps:\n",
        "    print(f\"  Max steps = {max_steps}\")\n",
        "else:\n",
        "    print(f\"  Epochs = {num_train_epochs}\")\n",
        "print(f\"  Learning rate = {learning_rate}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 10: Train Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nüèÉ Step 10: Starting training...\")\n",
        "trainer_stats = trainer.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 11: Training Statistics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "    print(f\"\\nüìà Training Statistics:\")\n",
        "    print(f\"  Runtime = {trainer_stats.metrics['train_runtime']:.2f} seconds ({trainer_stats.metrics['train_runtime']/60:.2f} minutes)\")\n",
        "    print(f\"  Peak reserved memory = {used_memory} GB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 12: Save Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if save_model:\n",
        "    print(f\"\\nüíæ Step 12: Saving model to {output_dir}...\")\n",
        "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
        "    model.save_pretrained(output_dir)\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "    print(f\"‚úì Model saved to {output_dir}\")\n",
        "    \n",
        "    # Optionally push to HuggingFace Hub\n",
        "    push_to_hub = os.environ.get(\"PUSH_TO_HUB\", \"\").lower() == \"true\"\n",
        "    hub_repo_id = os.environ.get(\"HUB_REPO_ID\")\n",
        "    if push_to_hub and hub_repo_id:\n",
        "        print(f\"\\nüì§ Pushing model to HuggingFace Hub: {hub_repo_id}...\")\n",
        "        model.push_to_hub(hub_repo_id, token=hf_token)\n",
        "        tokenizer.push_to_hub(hub_repo_id, token=hf_token)\n",
        "        print(f\"‚úì Model pushed to {hub_repo_id}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 13: Test Inference (Optional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if test_inference:\n",
        "    print(\"\\nüß™ Step 13: Testing inference...\")\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are an AI assistant that helps write professional emails.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Write an email to test@example.com about 'Project Update - Q4 Results'.\"},\n",
        "    ]\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\",\n",
        "        return_dict=True,\n",
        "        reasoning_effort=\"medium\",\n",
        "    ).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    \n",
        "    print(\"\\nGenerated email:\")\n",
        "    print(\"-\" * 60)\n",
        "    _ = model.generate(**inputs, max_new_tokens=256, streamer=TextStreamer(tokenizer))\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "print(\"\\n‚úÖ Fine-tuning complete!\")\n",
        "print(f\"Model saved to: {output_dir}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
